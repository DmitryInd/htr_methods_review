{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "287a2056",
   "metadata": {
    "cellId": "qxyebik3hhl3vpn7lv0f4x"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b692a",
   "metadata": {
    "cellId": "lc1q8r3ohqpj5plm8h73aq"
   },
   "source": [
    "#### Generating artificial dataset of cyrillic texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cbb16a",
   "metadata": {
    "cellId": "nws77e9taervuhl7rfwgg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: ../data/Peter's_notes/notes_train.csv\n"
     ]
    }
   ],
   "source": [
    "#!c1.8\n",
    "%run prepare_data.py \\\n",
    "    --data_csv_path \"../data/Peter's_notes/notes_train.csv\" \\\n",
    "    --output_pkl_name \"../data/Peter's_notes/train_dataset.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a286ad",
   "metadata": {
    "cellId": "wj6l7wpqfizz5p2bwn3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***************************************************** Loading Data *****************************************************\n",
      "************************************************** Model: ScrabbleGAN **************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count for Gs initialized parameters: 21695491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ScrabbleGAN(\n",
      "  (R): Recognizer(\n",
      "    (convs): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (output): Linear(in_features=512, out_features=73, bias=True)\n",
      "    (prob): LogSoftmax(dim=2)\n",
      "  )\n",
      "  (G): Generator(\n",
      "    (activation): ReLU()\n",
      "    (shared): identity()\n",
      "    (linear): SNLinear(in_features=2336, out_features=8192, bias=True)\n",
      "    (blocks): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): GBlock(\n",
      "          (activation): ReLU()\n",
      "          (conv1): SNConv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): SNConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv_sc): SNConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (bn1): ccbn(\n",
      "            out: 512, in: 32, cross_replica=False\n",
      "            (gain): SNLinear(in_features=32, out_features=512, bias=False)\n",
      "            (bias): SNLinear(in_features=32, out_features=512, bias=False)\n",
      "          )\n",
      "          (bn2): ccbn(\n",
      "            out: 256, in: 32, cross_replica=False\n",
      "            (gain): SNLinear(in_features=32, out_features=256, bias=False)\n",
      "            (bias): SNLinear(in_features=32, out_features=256, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): GBlock(\n",
      "          (activation): ReLU()\n",
      "          (conv1): SNConv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): SNConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv_sc): SNConv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (bn1): ccbn(\n",
      "            out: 256, in: 32, cross_replica=False\n",
      "            (gain): SNLinear(in_features=32, out_features=256, bias=False)\n",
      "            (bias): SNLinear(in_features=32, out_features=256, bias=False)\n",
      "          )\n",
      "          (bn2): ccbn(\n",
      "            out: 128, in: 32, cross_replica=False\n",
      "            (gain): SNLinear(in_features=32, out_features=128, bias=False)\n",
      "            (bias): SNLinear(in_features=32, out_features=128, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): GBlock(\n",
      "          (activation): ReLU()\n",
      "          (conv1): SNConv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): SNConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_sc): SNConv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (bn1): ccbn(\n",
      "            out: 128, in: 32, cross_replica=False\n",
      "            (gain): SNLinear(in_features=32, out_features=128, bias=False)\n",
      "            (bias): SNLinear(in_features=32, out_features=128, bias=False)\n",
      "          )\n",
      "          (bn2): ccbn(\n",
      "            out: 64, in: 32, cross_replica=False\n",
      "            (gain): SNLinear(in_features=32, out_features=64, bias=False)\n",
      "            (bias): SNLinear(in_features=32, out_features=64, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_layer): Sequential(\n",
      "      (0): bn()\n",
      "      (1): ReLU()\n",
      "      (2): SNConv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (D): Discriminator(\n",
      "    (activation): ReLU()\n",
      "    (blocks): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): DBlock(\n",
      "          (activation): ReLU()\n",
      "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "          (conv1): SNConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): SNConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv_sc): SNConv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): DBlock(\n",
      "          (activation): ReLU()\n",
      "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "          (conv1): SNConv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): SNConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv_sc): SNConv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): DBlock(\n",
      "          (activation): ReLU()\n",
      "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "          (conv1): SNConv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): SNConv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv_sc): SNConv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (3): ModuleList(\n",
      "        (0): DBlock(\n",
      "          (activation): ReLU()\n",
      "          (conv1): SNConv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): SNConv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear): SNLinear(in_features=1024, out_features=1, bias=True)\n",
      "    (embed): SNLinear(in_features=73, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count for Ds initialized parameters: 36363841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************************************* Training *******************************************************\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [1/60] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "G = -60.407, D = 437.015, R_real = 544.947, R_fake = 8.250,  :   1%|                 | 3/573 [00:24<1:18:39,  8.28s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 6.00 GiB total capacity; 4.92 GiB already allocated; 0 bytes free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Документы\\htr_methods_review\\ScrabbleGAN\\train.py:323\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    320\u001B[0m args \u001B[38;5;241m=\u001B[39m parser\u001B[38;5;241m.\u001B[39mparse_args()\n\u001B[0;32m    322\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(Config, args)\n\u001B[1;32m--> 323\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Документы\\htr_methods_review\\ScrabbleGAN\\train.py:237\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;66;03m# Forward + Backward + Optimize G\u001B[39;00m\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (i \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mtrain_gen_steps) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    236\u001B[0m     \u001B[38;5;66;03m# optimize generator\u001B[39;00m\n\u001B[1;32m--> 237\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize_G\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    239\u001B[0m \u001B[38;5;66;03m# Forward + Backward + Optimize D and R\u001B[39;00m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimize_D_R()\n",
      "File \u001B[1;32mD:\\Документы\\htr_methods_review\\ScrabbleGAN\\train.py:143\u001B[0m, in \u001B[0;36mTrainer.optimize_G\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_R_fake \u001B[38;5;241m=\u001B[39m a\u001B[38;5;241m.\u001B[39mdetach() \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_R_fake\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_G_total \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_G \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_R_fake\n\u001B[1;32m--> 143\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_G_total\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    144\u001B[0m grad_fake_R \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mgrad(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_R_fake, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mfake_img,\n\u001B[0;32m    145\u001B[0m                                   create_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    146\u001B[0m grad_fake_adv \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mgrad(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_G, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mfake_img,\n\u001B[0;32m    147\u001B[0m                                     create_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:363\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    356\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    357\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    361\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    362\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 363\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 6.00 GiB total capacity; 4.92 GiB already allocated; 0 bytes free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%run train.py \\\n",
    "    --data_pkl_path \"../data/Peter's_notes/train_dataset.pkl\" \\\n",
    "    --lexicon_path \"../data/Peter's_notes/notes_train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965dd5ec",
   "metadata": {
    "cellId": "otlsvnbqhjrv6250ifxhmk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%run generate_images.py \\\n",
    "    --checkpoint_path ./weights/model_checkpoint_epoch_30.pth.tar \\\n",
    "    --char_map_path \"../data/Peter's_notes/train_dataset.pkl\" \\\n",
    "    --num_imgs 100 \\\n",
    "    --lexicon_path \"../data/Peter's_notes/notes_train.txt\" \\\n",
    "    --output_path \"../data/generated_synth/Peter's_notes/train\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reserve",
   "language": "python",
   "name": "reserve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "notebookId": "a778ec01-acd2-4957-9387-3ef45e344512",
  "notebookPath": "htr_methods_review/ScrabbleGAN/generate_dataset.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}