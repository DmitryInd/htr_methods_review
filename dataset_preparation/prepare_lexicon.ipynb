{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "aulcjxw58alsxjkhvbnsq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/kernel/lib/python3.7/site-packages/ml_kernel/state/state.py\", line 99, in __eq__\n",
      "    self._unwrap()\n",
      "  File \"/kernel/lib/python3.7/site-packages/ml_kernel/state/state.py\", line 84, in _unwrap\n",
      "    raise Exception(\"variable not found\")\n",
      "Exception: variable not found\n",
      "Traceback (most recent call last):\n",
      "  File \"/kernel/lib/python3.7/site-packages/ml_kernel/state/state.py\", line 99, in __eq__\n",
      "    self._unwrap()\n",
      "  File \"/kernel/lib/python3.7/site-packages/ml_kernel/state/state.py\", line 84, in _unwrap\n",
      "    raise Exception(\"variable not found\")\n",
      "Exception: variable not found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "09z9osjm4vhsrna81i5fz8s",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"indenbomdmitry\"\n",
    "os.environ['KAGGLE_KEY'] = \"d71056824e379aa1756815ce2a658476\"\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "o13su44h5rlufabwo5qml",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clear_directory(path_to_dir: str):\n",
    "    if os.path.exists(path_to_dir):\n",
    "        shutil.rmtree(path_to_dir)\n",
    "    os.makedirs(path_to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "6rz7136tejkpwdnwfcvedq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_dataset_from_kaggle(kaggle_dataset_path: str, out_directory: str):\n",
    "    clear_directory(out_directory)\n",
    "    kaggle.api.dataset_download_files(kaggle_dataset_path,\n",
    "                                      path=out_directory,\n",
    "                                      unzip=True,\n",
    "                                      quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "bqw7j18u7zlme5cv0rovje",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bar_progress(current, total, width=80):\n",
    "    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
    "    # Don't use print() as it will print in new line every time.\n",
    "    sys.stdout.write(\"\\r\" + progress_message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def download_dataset_on_link(url, out_directory):\n",
    "    clear_directory(out_directory)\n",
    "    filename = wget.download(url,\n",
    "                             out=out_directory,\n",
    "                             bar=bar_progress)\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(out_directory)\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "p87a8tjihf4ey95v5i8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WordChecker:\n",
    "    def __init__(self, symbols: str, max_length):\n",
    "        self.alphabet = set(symbols)\n",
    "        self.max_len = max_length\n",
    "\n",
    "    def __call__(self, word: str) -> bool:\n",
    "        return self.alphabet.union(word) == self.alphabet and len(word) <= self.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellId": "k8a304h14tjkr80imnyyw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_sequence_to_lexicon(lexicon: list, text: str,  acceptable_symbols: str, max_length: int, avg_length: int):\n",
    "    word_checker = WordChecker(acceptable_symbols, max_length)\n",
    "    seq = \"\"\n",
    "    seq_length = 0\n",
    "    for word in text.split():\n",
    "        if not word_checker(word):\n",
    "            continue\n",
    "        if seq != \"\" and len(seq) + len(word) > seq_length:\n",
    "            lexicon.append(seq)\n",
    "            seq = \"\"\n",
    "        if seq == \"\":\n",
    "            seq_length = min(np.random.poisson(avg_length), max_length)\n",
    "            seq = word\n",
    "        else:\n",
    "            seq += ' ' + word\n",
    "    if seq != \"\":\n",
    "        lexicon.append(seq)\n",
    "\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "tq2d7zzao0qoxomxfj9di",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def write_lexicon(lexicon: list, path_to_file, mode='w'):\n",
    "    with open(path_to_file, mode, encoding=\"utf-8\") as the_file:\n",
    "        the_file.write('\\n'.join(lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "emwolqwaox56n80t7e12",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Preparing lexicon of indian english names for handwritten names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "acnuxoxojwrcsbfavzsi9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_output_length = 34\n",
    "avg_output_length = 7\n",
    "lexicon_size = 15000\n",
    "alphabet = \" !\\\"'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]_|}’№\"\n",
    "path_to_dataset = \"../data/lexicon-english-names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "bp1gp3scvqqhgjyo1wk8ij",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading indian-names-male-and-female.zip to ../data/lexicon-english-names\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 539k/539k [00:00<00:00, 4.69MB/s]\n"
     ]
    }
   ],
   "source": [
    "download_dataset_from_kaggle(\"kanchitank/indian-names-male-and-female\",\n",
    "                             path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "blzf2rrddfnhaboqh6th7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_to_dataset + \"/Names_dataset.csv\", encoding=\"utf-8\")[\"name\"]\n",
    "# Deleting empty rows\n",
    "data = data.dropna().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "fvz2jfx8kkdb2opmi37a3p",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts is 138409\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for sequence in data:\n",
    "    add_sequence_to_lexicon(texts, str(sequence).upper(), alphabet, max_output_length, avg_output_length)\n",
    "\n",
    "print(\"Number of texts is \" + str(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellId": "4g3ig1akib3j7xwefrfav",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subset of texts is 15000\n"
     ]
    }
   ],
   "source": [
    "sub_texts = random.sample(texts, min(lexicon_size, len(texts)))\n",
    "print(\"Number of subset of texts is \" + str(len(sub_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellId": "795h9fudt5smopa7u164pi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "write_lexicon(sub_texts, path_to_dataset + \"/names_lexicon.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "nurc4anqwrr7354leqqiyw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Preparing lexicon of russian news for cyrillic texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellId": "nsot34a844lkrif4ciad",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_output_length = 25\n",
    "avg_output_length = 7\n",
    "lexicon_size = 15000\n",
    "alphabet = \" !\\\"%(),-./0123456789:;?[]«»АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё\"\n",
    "path_to_dataset = \"../data/russian-news\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellId": "wvrc3riy9d7a8q6h5g2od",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading russian-news-2020.zip to ../data/russian-news\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19.9M/19.9M [00:00<00:00, 36.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "download_dataset_from_kaggle(\"vfomenko/russian-news-2020\",\n",
    "                             path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellId": "z8l0r0pyu6et9wry85e03",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_to_dataset + \"/news.csv\", encoding=\"utf-8\")[\"text\"]\n",
    "# Deleting empty rows\n",
    "data = data.dropna().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellId": "aailvf5vmb8ailpkw5a6z7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts is 4261421\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for sequence in data:\n",
    "    add_sequence_to_lexicon(texts, str(sequence), alphabet, max_output_length, avg_output_length)\n",
    "\n",
    "print(\"Number of texts is \" + str(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellId": "00mlbj7cc9ebr64irtgtxq87",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subset of texts is 15000\n"
     ]
    }
   ],
   "source": [
    "sub_texts = random.sample(texts, min(lexicon_size, len(texts)))\n",
    "print(\"Number of subset of texts is \" + str(len(sub_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellId": "z5czz89rhl3auxakfd1lw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "write_lexicon(sub_texts, path_to_dataset + \"/news.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "6lpktgvlho99oo75tvxb9h",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Preparing lexicon for Peter's notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellId": "1pfyoyjwwh4ctav431fm7i",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_output_length = 70\n",
    "avg_output_length = 28\n",
    "lexicon_size = 10000\n",
    "alphabet = \" ()+/0123456789[]abdefghiklmnoprstu|×ǂабвгдежзийклмнопрстуфхцчшщъыьэюяѣ–⊕⊗\"\n",
    "path_to_dataset = \"../data/Peter's_notes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellId": "x1xj3q76bpyqi9k2q21fa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_to_dataset + \"/notes_train.csv\", encoding=\"utf-8\")[\"text\"]\n",
    "# Deleting empty rows\n",
    "data = data.dropna().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellId": "0fli81lm18evfaybgol3ry5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts is 5195\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "words = ' '.join(data).split()\n",
    "random.shuffle(words)\n",
    "words = ' '.join(words)\n",
    "add_sequence_to_lexicon(texts, words, alphabet, max_output_length, avg_output_length)\n",
    "\n",
    "print(\"Number of texts is \" + str(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellId": "ey5ungqlkss4grz15oj804",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subset of texts is 5195\n"
     ]
    }
   ],
   "source": [
    "sub_texts = random.sample(texts, min(lexicon_size, len(texts)))\n",
    "print(\"Number of subset of texts is \" + str(len(sub_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellId": "x7k8b1rc4ws9ycqyhgxomi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "write_lexicon(sub_texts, path_to_dataset + \"/notes_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellId": "7oe729msyhpskx3ij7nuaq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subset of texts is 25000\n"
     ]
    }
   ],
   "source": [
    "target_test_volume = 25000\n",
    "vocabulary = words.split()\n",
    "gen_lexicon = []\n",
    "while len(gen_lexicon) < target_test_volume:\n",
    "    random.shuffle(vocabulary)\n",
    "    words = ' '.join(vocabulary)\n",
    "    add_sequence_to_lexicon(gen_lexicon, words, alphabet, max_output_length, avg_output_length)\n",
    "\n",
    "gen_lexicon = random.sample(gen_lexicon, min(target_test_volume, len(gen_lexicon)))\n",
    "print(\"Number of subset of texts is \" + str(len(gen_lexicon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellId": "r9a9sg9gmcdrne4933xm"
   },
   "outputs": [],
   "source": [
    "write_lexicon(gen_lexicon, path_to_dataset + \"/synthetic_notes.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "011b14b1-3fcc-4e38-be53-9295388bd2af",
  "notebookPath": "htr_methods_review/dataset_preparation/prepare_lexicon.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
