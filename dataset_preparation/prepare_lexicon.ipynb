{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellId": "kd45cs947rqx3ncj6rvchb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "833vklg257urig56hf5oel",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"indenbomdmitry\"\n",
    "os.environ['KAGGLE_KEY'] = \"d71056824e379aa1756815ce2a658476\"\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "7hatt12c4nfsuascwrtvrb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clear_directory(path_to_dir: str):\n",
    "    if os.path.exists(path_to_dir):\n",
    "        shutil.rmtree(path_to_dir)\n",
    "    os.makedirs(path_to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "opagrpqcduc57cfpclw3xa",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_dataset_from_kaggle(kaggle_dataset_path: str, out_directory: str):\n",
    "    clear_directory(out_directory)\n",
    "    kaggle.api.dataset_download_files(kaggle_dataset_path,\n",
    "                                      path=out_directory,\n",
    "                                      unzip=True,\n",
    "                                      quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "4725xdp3evwhnhqrsozk0o",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bar_progress(current, total, width=80):\n",
    "    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
    "    # Don't use print() as it will print in new line every time.\n",
    "    sys.stdout.write(\"\\r\" + progress_message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def download_dataset_on_link(url, out_directory):\n",
    "    clear_directory(out_directory)\n",
    "    filename = wget.download(url,\n",
    "                             out=out_directory,\n",
    "                             bar=bar_progress)\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(out_directory)\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "zdc976pbbx5d69w5yj21n",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def shuffle_texts(texts_list: list, max_words=7) -> list:\n",
    "    words = ' '.join(texts_list).split()\n",
    "    random.shuffle(words)\n",
    "    sequences = []\n",
    "    seq = \"\"\n",
    "    length = random.randint(1, max_words)\n",
    "    for word in words:\n",
    "        if length == 0:\n",
    "            sequences.append(seq)\n",
    "            length = random.randint(1, max_words)\n",
    "            seq = \"\"\n",
    "        length -= 1\n",
    "        if seq == \"\":\n",
    "            seq = word\n",
    "        else:\n",
    "            seq += ' ' + word\n",
    "    if seq != \"\":\n",
    "        sequences.append(seq)\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellId": "mug7p5v633jsbiyj4oq7wg",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WordChecker:\n",
    "    def __init__(self, symbols: str, max_length):\n",
    "        self.alphabet = set(symbols)\n",
    "        self.max_len = max_length\n",
    "\n",
    "    def __call__(self, word: str) -> bool:\n",
    "        return self.alphabet.union(word) == self.alphabet and len(word) <= self.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "jk0afyw4m8jihwz51a39mb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_sequence_to_lexicon(lexicon: list, text: str,  acceptable_symbols: str, max_length: int):\n",
    "    word_checker = WordChecker(acceptable_symbols, max_length)\n",
    "    seq = \"\"\n",
    "    for word in text.split():\n",
    "        if not word_checker(word):\n",
    "            continue\n",
    "        if len(seq) + len(word) > max_length:\n",
    "            lexicon.append(seq)\n",
    "            seq = \"\"\n",
    "        if seq == \"\":\n",
    "            seq = word\n",
    "        else:\n",
    "            seq += ' ' + word\n",
    "    if seq != \"\":\n",
    "        lexicon.append(seq)\n",
    "\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "lb4y2jqw8arbs6p3mpr7g5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def write_lexicon(lexicon: list, path_to_file, mode='w'):\n",
    "    with open(path_to_file, mode, encoding=\"utf-8\") as the_file:\n",
    "        the_file.write('\\n'.join(lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "5ohcojf2vcv74fmpvwey",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Preparing lexicon of indian english names for handwritten names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "2sjw5s39uhr9gesp5ncu0p",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_length = 40\n",
    "lexicon_size = 15000\n",
    "alphabet = \" !\\\"'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]_|}’№\"\n",
    "path_to_dataset = \"../data/lexicon-english-names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "pt5djm0dgvb668kmjn3z9f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading indian-names-male-and-female.zip to ../data/lexicon-english-names\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 539k/539k [00:00<00:00, 5.46MB/s]\n"
     ]
    }
   ],
   "source": [
    "download_dataset_from_kaggle(\"kanchitank/indian-names-male-and-female\",\n",
    "                             path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "yi8q3pi632n84a1m2pe0zg",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_to_dataset + \"/Names_dataset.csv\", encoding=\"utf-8\")[\"name\"]\n",
    "# Deleting empty rows\n",
    "data = data.dropna().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellId": "t1oaal5gpszc7i8e1kdw",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts is 125093\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for sequence in data:\n",
    "    add_sequence_to_lexicon(texts, str(sequence).upper(), alphabet, output_length)\n",
    "\n",
    "print(\"Number of texts is \" + str(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellId": "75cssf48nkkrc42ijwnzd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subset of texts is 15000\n"
     ]
    }
   ],
   "source": [
    "sub_texts = random.sample(texts, min(lexicon_size, len(texts)))\n",
    "print(\"Number of subset of texts is \" + str(len(sub_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellId": "grdjh7h2vzptj1e5hnllre",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "write_lexicon(sub_texts, path_to_dataset + \"/names_lexicon.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ra38mnqj8mjd7v9iimaw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Preparing lexicon of russian news for cyrillic texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellId": "jihwm1llhj3v3oqyci1wj",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_length = 40\n",
    "lexicon_size = 15000\n",
    "alphabet = \" !\\\"%(),-./0123456789:;?[]«»АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё\"\n",
    "path_to_dataset = \"../data/russian-news\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellId": "lbn5w2pfc1ttajxkisg5b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading russian-news-2020.zip to ../data/russian-news\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19.9M/19.9M [00:00<00:00, 59.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "download_dataset_from_kaggle(\"vfomenko/russian-news-2020\",\n",
    "                             path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellId": "rvd8jzu295l9b3wp7gzrr7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_to_dataset + \"/news.csv\", encoding=\"utf-8\")[\"text\"]\n",
    "# Deleting empty rows\n",
    "data = data.dropna().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellId": "x0o678zr09jr63ulxpw6g",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts is 976869\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for sequence in data:\n",
    "    add_sequence_to_lexicon(texts, str(sequence), alphabet, output_length)\n",
    "\n",
    "print(\"Number of texts is \" + str(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellId": "asfyogkqukdgeocei43z5c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subset of texts is 15000\n"
     ]
    }
   ],
   "source": [
    "sub_texts = random.sample(texts, min(lexicon_size, len(texts)))\n",
    "print(\"Number of subset of texts is \" + str(len(sub_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellId": "xdtubb9qvcijxf8ozlas6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "write_lexicon(sub_texts, path_to_dataset + \"/news.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "1hy2c5yjfmo1yn5dasimys",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Preparing lexicon for Peter's notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellId": "oa9496niweaetrpdrrrctr",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_length = 70\n",
    "lexicon_size = 10000\n",
    "alphabet = \" ()+/0123456789[]abdefghiklmnoprstu|×ǂабвгдежзийклмнопрстуфхцчшщъыьэюяѣ–⊕⊗\"\n",
    "path_to_dataset = \"../data/Peter's_notes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellId": "0e8ar8nndz9flz2ex2u35g8",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_to_dataset + \"/notes_train.csv\", encoding=\"utf-8\")[\"text\"]\n",
    "# Deleting empty rows\n",
    "data = data.dropna().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellId": "9nltocilkrk352unhrvp7i",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts is 4817\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "shuffled_data = shuffle_texts(data, max_words=10)\n",
    "for sequence in shuffled_data:\n",
    "    add_sequence_to_lexicon(texts, str(sequence), alphabet, output_length)\n",
    "\n",
    "print(\"Number of texts is \" + str(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellId": "rvq2adbauvlzmmelaimz",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subset of texts is 4817\n"
     ]
    }
   ],
   "source": [
    "sub_texts = random.sample(texts, min(lexicon_size, len(texts)))\n",
    "print(\"Number of subset of texts is \" + str(len(sub_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellId": "ldtt1459g1vkqfkprqs8h",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "write_lexicon(sub_texts, path_to_dataset + \"/notes_train.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "011b14b1-3fcc-4e38-be53-9295388bd2af",
  "notebookPath": "htr_methods_review/dataset_preparation/prepare_lexicon.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
