{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"indenbomdmitry\"\n",
    "os.environ['KAGGLE_KEY'] = \"d71056824e379aa1756815ce2a658476\"\n",
    "import kaggle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def clear_directory(path_to_dir: str):\n",
    "    if os.path.exists(path_to_dir):\n",
    "        shutil.rmtree(path_to_dir)\n",
    "    os.makedirs(path_to_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def download_dataset_from_kaggle(kaggle_dataset_path: str, out_directory: str):\n",
    "    clear_directory(out_directory)\n",
    "    kaggle.api.dataset_download_files(kaggle_dataset_path,\n",
    "                                      path=out_directory,\n",
    "                                      unzip=True,\n",
    "                                      quiet=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def bar_progress(current, total, width=80):\n",
    "    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
    "    # Don't use print() as it will print in new line every time.\n",
    "    sys.stdout.write(\"\\r\" + progress_message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def download_dataset_on_link(url, out_directory):\n",
    "    clear_directory(out_directory)\n",
    "    filename = wget.download(url,\n",
    "                             out=out_directory,\n",
    "                             bar=bar_progress)\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(out_directory)\n",
    "    os.remove(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class WordChecker:\n",
    "    def __init__(self, symbols: str, max_length):\n",
    "        self.alphabet = set(symbols)\n",
    "        self.max_len = max_length\n",
    "\n",
    "    def __call__(self, word: str) -> bool:\n",
    "        return self.alphabet.union(word) == self.alphabet and len(word) <= self.max_len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def add_sequence_to_lexicon(lexicon: list, text: str,  acceptable_symbols: str, max_length: int, avg_length: int):\n",
    "    word_checker = WordChecker(acceptable_symbols, max_length)\n",
    "    seq = \"\"\n",
    "    seq_length = 0\n",
    "    for word in text.split():\n",
    "        if not word_checker(word):\n",
    "            continue\n",
    "        if seq != \"\" and len(seq) + len(word) > seq_length:\n",
    "            lexicon.append(seq)\n",
    "            seq = \"\"\n",
    "        if seq == \"\":\n",
    "            seq_length = min(np.random.poisson(avg_length), max_length)\n",
    "            seq = word\n",
    "        else:\n",
    "            seq += ' ' + word\n",
    "    if seq != \"\":\n",
    "        lexicon.append(seq)\n",
    "\n",
    "    return lexicon"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def write_lexicon(lexicon: list, path_to_file, mode='w'):\n",
    "    with open(path_to_file, mode, encoding=\"utf-8\") as the_file:\n",
    "        the_file.write('\\n'.join(lexicon))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preparing lexicon of indian english names for handwritten names:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "max_output_length = 34\n",
    "avg_output_length = 7\n",
    "lexicon_size = 15000\n",
    "alphabet = \" !\\\"'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]_|}’№\"\n",
    "path_to_dataset = \"../data/lexicon-english-names\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading indian-names-male-and-female.zip to ../data/lexicon-english-names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 539k/539k [00:00<00:00, 5.80MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "download_dataset_from_kaggle(\"kanchitank/indian-names-male-and-female\",\n",
    "                             path_to_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_to_dataset + \"/Names_dataset.csv\", encoding=\"utf-8\")[\"name\"]\n",
    "# Deleting empty rows\n",
    "data = data.dropna().to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts is 138436\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for sequence in data:\n",
    "    add_sequence_to_lexicon(texts, str(sequence).upper(), alphabet, max_output_length, avg_output_length)\n",
    "\n",
    "print(\"Number of texts is \" + str(len(texts)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subset of texts is 15000\n"
     ]
    }
   ],
   "source": [
    "sub_texts = random.sample(texts, min(lexicon_size, len(texts)))\n",
    "print(\"Number of subset of texts is \" + str(len(sub_texts)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "write_lexicon(sub_texts, path_to_dataset + \"/names_lexicon.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preparing lexicon of russian news for cyrillic texts:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "max_output_length = 25\n",
    "avg_output_length = 7\n",
    "lexicon_size = 15000\n",
    "alphabet = \" !\\\"%(),-./0123456789:;?[]«»АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё\"\n",
    "path_to_dataset = \"../data/russian-news\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading russian-news-2020.zip to ../data/russian-news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19.9M/19.9M [00:01<00:00, 18.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "download_dataset_from_kaggle(\"vfomenko/russian-news-2020\",\n",
    "                             path_to_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_to_dataset + \"/news.csv\", encoding=\"utf-8\")[\"text\"]\n",
    "# Deleting empty rows\n",
    "data = data.dropna().to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts is 4261487\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for sequence in data:\n",
    "    add_sequence_to_lexicon(texts, str(sequence), alphabet, max_output_length, avg_output_length)\n",
    "\n",
    "print(\"Number of texts is \" + str(len(texts)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subset of texts is 15000\n"
     ]
    }
   ],
   "source": [
    "sub_texts = random.sample(texts, min(lexicon_size, len(texts)))\n",
    "print(\"Number of subset of texts is \" + str(len(sub_texts)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "write_lexicon(sub_texts, path_to_dataset + \"/news.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preparing lexicon for Peter's notes:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "max_output_length = 70\n",
    "avg_output_length = 28\n",
    "lexicon_size = 10000\n",
    "alphabet = \" ()+/0123456789[]abdefghiklmnoprstu|×ǂабвгдежзийклмнопрстуфхцчшщъыьэюяѣ–⊕⊗\"\n",
    "path_to_dataset = \"../data/Peter's_notes\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_to_dataset + \"/notes_train.csv\", encoding=\"utf-8\")[\"text\"]\n",
    "# Deleting empty rows\n",
    "data = data.dropna().to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts is 5211\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "words = ' '.join(data).split()\n",
    "random.shuffle(words)\n",
    "words = ' '.join(words)\n",
    "add_sequence_to_lexicon(texts, words, alphabet, max_output_length, avg_output_length)\n",
    "\n",
    "print(\"Number of texts is \" + str(len(texts)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subset of texts is 5211\n"
     ]
    }
   ],
   "source": [
    "sub_texts = random.sample(texts, min(lexicon_size, len(texts)))\n",
    "print(\"Number of subset of texts is \" + str(len(sub_texts)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "write_lexicon(sub_texts, path_to_dataset + \"/notes_train.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}